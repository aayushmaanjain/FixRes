{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 30 11:51:05 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   38C    P0    21W / 300W |      0MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "  \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "  with torch.no_grad():\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "      correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "      res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "class ProgressMeter(object):\n",
    "  def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "    self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "    self.meters = meters\n",
    "    self.prefix = prefix\n",
    "\n",
    "  def display(self, batch):\n",
    "    entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "    entries += [str(meter) for meter in self.meters]\n",
    "    print('\\t'.join(entries))\n",
    "\n",
    "  def _get_batch_fmtstr(self, num_batches):\n",
    "    num_digits = len(str(num_batches // 1))\n",
    "    fmt = '{:' + str(num_digits) + 'd}'\n",
    "    return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "class AverageMeter(object):\n",
    "  \"\"\"Computes and stores the average and current value\"\"\"\n",
    "  def __init__(self, name, fmt=':f'):\n",
    "    self.name = name\n",
    "    self.fmt = fmt\n",
    "    self.reset()\n",
    "    self.epoch_sum = 0\n",
    "    self.epoch_count = 0\n",
    "    self.epoch_avg = 0\n",
    "\n",
    "  def reset(self):\n",
    "#     self.val = 0\n",
    "    self.avg = 0\n",
    "    self.sum = 0\n",
    "    self.count = 0\n",
    "\n",
    "  def update(self, val, n=1):\n",
    "    self.val = val\n",
    "    self.sum += val * n\n",
    "    self.count += n\n",
    "    self.avg = self.sum / self.count\n",
    "    self.epoch_sum += val * n\n",
    "    self.epoch_count += n\n",
    "    self.epoch_avg = self.epoch_sum / self.epoch_count\n",
    "    \n",
    "  def __str__(self):\n",
    "    fmtstr = '{name} {avg' + self.fmt + '} ({epoch_avg' + self.fmt + '})'\n",
    "    return fmtstr.format(**self.__dict__)\n",
    "  \n",
    "def save_checkpoint(state, is_best, filename='checkpoint_conv.pth.tar'):\n",
    "  torch.save(state, filename)\n",
    "  if is_best:\n",
    "    shutil.copyfile(filename, 'model_best_conv.pth.tar')\n",
    "    \n",
    "def imshow(img):\n",
    "  unnormalize = transforms.Normalize((-0.4914/0.247, -0.4822/0.243, -0.4465/0.261), (1/0.247, 1/0.243, 1/0.261))\n",
    "  img = unnormalize(img)\n",
    "  npimg = img.numpy()\n",
    "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(model, input_size, batch_size=-1, device=\"cuda\"):\n",
    "\n",
    "    def register_hook(module):\n",
    "\n",
    "        def hook(module, input, output):\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "\n",
    "            m_key = \"%s-%i\" % (class_name, module_idx + 1)\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in output\n",
    "                ]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "            not isinstance(module, nn.Sequential)\n",
    "            and not isinstance(module, nn.ModuleList)\n",
    "            and not (module == model)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    device = device.lower()\n",
    "    assert device in [\n",
    "        \"cuda\",\n",
    "        \"cpu\",\n",
    "    ], \"Input device is not valid, please specify 'cuda' or 'cpu'\"\n",
    "\n",
    "    if device == \"cuda\" and torch.cuda.is_available():\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    # multiple inputs to the network\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    # batch_size of 2 for batchnorm\n",
    "    x = [torch.rand(2, *in_size).type(dtype) for in_size in input_size]\n",
    "    print(x[0].shape)\n",
    "    # print(type(x[0]))\n",
    "\n",
    "    # create properties\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    # register hook\n",
    "    model.apply(register_hook)\n",
    "\n",
    "    # make a forward pass\n",
    "    # print(x.shape)\n",
    "    model(*x)\n",
    "\n",
    "    # remove these hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer (type)\", \"Output Shape\", \"Param #\")\n",
    "    print(line_new)\n",
    "    print(\"================================================================\")\n",
    "    total_params = 0\n",
    "    total_output = 0\n",
    "    trainable_params = 0\n",
    "    for layer in summary:\n",
    "        # input_shape, output_shape, trainable, nb_params\n",
    "        line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "            layer,\n",
    "            str(summary[layer][\"output_shape\"]),\n",
    "            \"{0:,}\".format(summary[layer][\"nb_params\"]),\n",
    "        )\n",
    "        total_params += summary[layer][\"nb_params\"]\n",
    "        total_output += np.prod(summary[layer][\"output_shape\"])\n",
    "        if \"trainable\" in summary[layer]:\n",
    "            if summary[layer][\"trainable\"] == True:\n",
    "                trainable_params += summary[layer][\"nb_params\"]\n",
    "        print(line_new)\n",
    "\n",
    "    # assume 4 bytes/number (float on cuda).\n",
    "    total_input_size = abs(np.prod(input_size) * batch_size * 4. / (1024 ** 2.))\n",
    "    total_output_size = abs(2. * total_output * 4. / (1024 ** 2.))  # x2 for gradients\n",
    "    total_params_size = abs(total_params.numpy() * 4. / (1024 ** 2.))\n",
    "    total_size = total_params_size + total_output_size + total_input_size\n",
    "\n",
    "    print(\"================================================================\")\n",
    "    print(\"Total params: {0:,}\".format(total_params))\n",
    "    print(\"Trainable params: {0:,}\".format(trainable_params))\n",
    "    print(\"Non-trainable params: {0:,}\".format(total_params - trainable_params))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Input size (MB): %0.2f\" % total_input_size)\n",
    "    print(\"Forward/backward pass size (MB): %0.2f\" % total_output_size)\n",
    "    print(\"Params size (MB): %0.2f\" % total_params_size)\n",
    "    print(\"Estimated Total Size (MB): %0.2f\" % total_size)\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    # return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FixRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/facebookresearch/FixRes.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/FixRes\n",
      "CODE_OF_CONDUCT.md\timage\t\t\t  main_extract.py\n",
      "CONTRIBUTING.md\t\timnet_evaluate\t\t  main_finetune.py\n",
      "Contest\t\t\timnet_extract\t\t  main_resnet50_scratch.py\n",
      "LICENSE.md\t\timnet_finetune\t\t  requirements.txt\n",
      "README.md\t\timnet_resnet50_scratch\t  setup.py\n",
      "ResNeXt_101_32x48d.pth\tlogs\t\t\t  transforms_v2.py\n",
      "data\t\t\tmain_evaluate_imnet.py\n",
      "hubconf.py\t\tmain_evaluate_softmax.py\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/FixRes\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt  update\n",
    "# !apt install wget\n",
    "# !wget https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Pretrained_Models/ResNeXt_101_32x48d.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from imnet_evaluate.resnext_wsl import resnext101_32x48d_wsl\n",
    "\n",
    "model = resnext101_32x48d_wsl(progress=True)\n",
    "\n",
    "pretrained_dict = torch.load('ResNeXt_101_32x48d.pth', map_location='cpu')['model']\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "for k in model_dict.keys():\n",
    "  if(('module.'+k) in pretrained_dict.keys()):\n",
    "    model_dict[k] = pretrained_dict.get(('module.'+k))\n",
    "    \n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1281167\n",
      "    Root location: /workspace/data/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=365, interpolation=PIL.Image.BILINEAR, largest=False)\n",
      "               CenterCrop(size=(320, 320))\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "## Settings\n",
    "batch_size = 4\n",
    "val_ratio = 10000/50000\n",
    "batch_print_freq = 500\n",
    "start_epoch = 0\n",
    "# epochs = 1\n",
    "\n",
    "###################################################\n",
    "## Load Data\n",
    "# dataloaders = {}\n",
    "# dataloaders['train'], dataloaders['val'] = get_train_val_loaders('./data', batch_size, val_ratio)\n",
    "# trainloader, _ =  get_train_val_loaders('./data', batch_size, val_ratio)\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "from imnet_finetune.transforms import get_transforms\n",
    "transformation = get_transforms(input_size=320,test_size=320, kind='full', crop=True, need=('train', 'val'), backbone=None)\n",
    "trainset = torchvision.datasets.ImageFolder('/workspace/data/train', transform=transformation['val'])\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, num_workers=2)\n",
    "print(trainset)\n",
    "\n",
    "###################################################\n",
    "## Load Model\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define/load model\n",
    "# num_ftrs = model.fc.in_features\n",
    "# model.fc = nn.Linear(num_ftrs, 10)\n",
    "# Send model to GPU\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function (criterion) and optimizer and LR scheduler\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "# NOTE: define optimizer after sending model to GPU. May lead to error otherwise.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) \n",
    "#   lrscheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDNN_LOGINFO_DBG=1\n",
      "env: CUDNN_LOGDEST_DBG=/workspace/FixRes/logs/cudnn1.log\n"
     ]
    }
   ],
   "source": [
    "%env CUDNN_LOGINFO_DBG=1\n",
    "%env CUDNN_LOGDEST_DBG=/workspace/FixRes/logs/cudnn1.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * TRAIN: Acc@1 0.000 Acc@5 25.000\n"
     ]
    }
   ],
   "source": [
    "## Profiling Training on GPU\n",
    "losses = AverageMeter('Loss', ':.4e')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "# set to train mode\n",
    "model.train()\n",
    "\n",
    "trainiter = iter(trainloader)\n",
    "# specify which batch you want to profile\n",
    "batches = 1\n",
    "isProfile = False\n",
    "for i in range(batches):\n",
    "    images, target = trainiter.next()\n",
    "    images = images.to(device)\n",
    "    target = target.to(device)\n",
    "  \n",
    "#     if i == (batches-1):\n",
    "#         isProfile = True\n",
    "    \n",
    "#     with torch.autograd.profiler.profile(enabled=isProfile,use_cuda=True) as prof:\n",
    "    output = model(images)\n",
    "    loss = criterion(output, target)\n",
    "  # compute gradients and do kprop \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "   # measure accuracy and record loss\n",
    "    acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "    losses.update(loss.item(), images.size(0))\n",
    "    top1.update(acc1[0], images.size(0))\n",
    "    top5.update(acc5[0], images.size(0))\n",
    "    \n",
    "    print(' * TRAIN: Acc@1 {top1.epoch_avg:.3f} Acc@5 {top5.epoch_avg:.3f}'.format(top1=top1, top5=top5))\n",
    "    \n",
    "# print(prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 28 06:22:18 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   46C    P0    56W / 300W |  15559MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1281167\n",
      "    Root location: /workspace/data/train\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.ImageFolder('/workspace/data/train')\n",
    "print(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 12288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(12288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(12288, 12288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(12288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(12288, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 12288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(12288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(12288, 12288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(12288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(12288, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 12288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(12288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(12288, 12288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(12288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(12288, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "blocks = []\n",
    "with open('logs/cudnn1.log', 'r') as logfile:\n",
    "    while(True):\n",
    "        line = logfile.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        # do stuff\n",
    "        if 'cudnnConvolutionForward()' in line:\n",
    "            block = line\n",
    "            line = logfile.readline()\n",
    "            while(line.strip()):\n",
    "                block += line\n",
    "                line = logfile.readline()\n",
    "            blocks.append(block)\n",
    "print(len(blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"logs/convfwd.txt\", 'w') as file:\n",
    "#     for i,block in enumerate(blocks):\n",
    "#         file.write(str(i+1) + '\\n')\n",
    "#         file.write(block)\n",
    "#         if i < len(blocks)-1:\n",
    "#             file.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 4, 'c': 3, 'h': 320, 'w': 320, 'k': 64, 'f_h': 7, 'f_w': 7, 'pad_h': 3, 'pad_w': 3, 'stride_h': 2, 'stride_w': 2, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 64, 'h': 80, 'w': 80, 'k': 1536, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 1536, 'h': 80, 'w': 80, 'k': 1536, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1536, 'h': 80, 'w': 80, 'k': 256, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 64, 'h': 80, 'w': 80, 'k': 256, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 256, 'h': 80, 'w': 80, 'k': 1536, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 1536, 'h': 80, 'w': 80, 'k': 1536, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1536, 'h': 80, 'w': 80, 'k': 256, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 256, 'h': 80, 'w': 80, 'k': 1536, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 1536, 'h': 80, 'w': 80, 'k': 1536, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1536, 'h': 80, 'w': 80, 'k': 256, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 256, 'h': 80, 'w': 80, 'k': 3072, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 3072, 'h': 80, 'w': 80, 'k': 3072, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 2, 'stride_w': 2, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 3072, 'h': 40, 'w': 40, 'k': 512, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 256, 'h': 80, 'w': 80, 'k': 512, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 2, 'stride_w': 2, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 512, 'h': 40, 'w': 40, 'k': 3072, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 3072, 'h': 40, 'w': 40, 'k': 3072, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 3072, 'h': 40, 'w': 40, 'k': 512, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 512, 'h': 40, 'w': 40, 'k': 3072, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 3072, 'h': 40, 'w': 40, 'k': 3072, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 3072, 'h': 40, 'w': 40, 'k': 512, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 512, 'h': 40, 'w': 40, 'k': 3072, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 3072, 'h': 40, 'w': 40, 'k': 3072, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 3072, 'h': 40, 'w': 40, 'k': 512, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 512, 'h': 40, 'w': 40, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 40, 'w': 40, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 2, 'stride_w': 2, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 512, 'h': 40, 'w': 40, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 2, 'stride_w': 2, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 6144, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 6144, 'h': 20, 'w': 20, 'k': 1024, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 12288, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM (1)'}\n",
      "{'n': 4, 'c': 12288, 'h': 20, 'w': 20, 'k': 12288, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 2, 'stride_w': 2, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 12288, 'h': 10, 'w': 10, 'k': 2048, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 1024, 'h': 20, 'w': 20, 'k': 2048, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 2, 'stride_w': 2, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 2048, 'h': 10, 'w': 10, 'k': 12288, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 12288, 'h': 10, 'w': 10, 'k': 12288, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 12288, 'h': 10, 'w': 10, 'k': 2048, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 2048, 'h': 10, 'w': 10, 'k': 12288, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 12288, 'h': 10, 'w': 10, 'k': 12288, 'f_h': 3, 'f_w': 3, 'pad_h': 1, 'pad_w': 1, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 32, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "{'n': 4, 'c': 12288, 'h': 10, 'w': 10, 'k': 2048, 'f_h': 1, 'f_w': 1, 'pad_h': 0, 'pad_w': 0, 'stride_h': 1, 'stride_w': 1, 'dil_h': 1, 'dil_w': 1, 'groupCount': 1, 'fwd_algo': 'CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM (0)'}\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "convsInfo = []\n",
    "for i, block in enumerate(blocks):\n",
    "    conv = {}\n",
    "    \n",
    "    lines = block.split('\\n')\n",
    "    \n",
    "    ## INPUT INFORMATION\n",
    "    xDescLine = [x for x in range(len(lines)) if 'xDesc' in lines[x]]\n",
    "    assert len(xDescLine) == 1, \"multiple 'xDesc' in block:{}\".format(i+1)\n",
    "    xDescLine = xDescLine[0]\n",
    "    \n",
    "    xPatter = re.compile(r'i!\\s+dimA: type=int; val=\\[(?P<n>\\d+),(?P<c>\\d+),(?P<h>\\d+),(?P<w>\\d+)\\];')\n",
    "    # Note: input information is in the 3rd line after xDesc key\n",
    "    xInfo = xPatter.match(lines[xDescLine + 3])\n",
    "    if xInfo:\n",
    "        conv['n'] = int(xInfo.group('n'))\n",
    "        conv['c'] = int(xInfo.group('c'))\n",
    "        conv['h'] = int(xInfo.group('h'))\n",
    "        conv['w'] = int(xInfo.group('w'))\n",
    "    else:\n",
    "        print(\"ERROR: Input dimensions not found at expected spot for block:{}\".format(i+1))\n",
    "        conv['n'] = conv['c'] = conv['h'] = conv['w'] = -1\n",
    "        \n",
    "    ## Filter Information\n",
    "    wDescLine = [x for x in range(len(lines)) if 'wDesc' in lines[x]]\n",
    "    assert len(wDescLine) == 1, \"multiple 'wDesc' in block:{}\".format(i+1)\n",
    "    wDescLine = wDescLine[0]\n",
    "    \n",
    "    wPattern = re.compile(r'i!\\s+dimA: type=int; val=\\[(?P<k>\\d+),(?P<i_prime>\\d+),(?P<f_h>\\d+),(?P<f_w>\\d+)\\];')\n",
    "    # NOTE: filter information is in the 4th line after wDesc key.\n",
    "    wInfo = wPattern.match(lines[wDescLine + 4])\n",
    "    if wInfo:\n",
    "#         conv['inDim'] = int(wInfo.group('i_prime')) # this is actually (inDim/groupCount)\n",
    "        conv['k'] = int(wInfo.group('k'))\n",
    "        conv['f_h'] = int(wInfo.group('f_h'))\n",
    "        conv['f_w'] = int(wInfo.group('f_w'))\n",
    "    else:\n",
    "        print(\"ERROR: Filter dimensions not found at expected spot for block:{}.\".format(i+1))\n",
    "        conv['k'] = conv['f_h'] = conv['f_w'] = -1\n",
    "    \n",
    "    # Convolution Info\n",
    "    convDescLine = [x for x in range(len(lines)) if 'convDesc' in lines[x]]\n",
    "    assert len(convDescLine) == 1, \"multiple 'convDesc' in block:{}\".format(i+1)\n",
    "    convDescLine = convDescLine[0]\n",
    "    \n",
    "    # Padding\n",
    "    padPattern = re.compile(r'i!\\s+padA: type=int; val=\\[(?P<pad_h>\\d+),(?P<pad_w>\\d+)\\];')\n",
    "    # NOTE: padding information is in the 6th line after convDesc key.\n",
    "    padInfo = padPattern.match(lines[convDescLine + 6])\n",
    "    if padInfo:\n",
    "        conv['pad_h'] = int(padInfo.group('pad_h'))\n",
    "        conv['pad_w'] = int(padInfo.group('pad_w'))\n",
    "    else:\n",
    "        print(\"ERROR: Padding dimensions not found at expected spot for block:{}.\".format(i+1))\n",
    "        conv['pad_h'] = conv['pad_w'] = -1\n",
    "    \n",
    "    # Stride\n",
    "    stridePattern = re.compile(r'i!\\s+strideA: type=int; val=\\[(?P<stride_h>\\d+),(?P<stride_w>\\d+)\\];')\n",
    "    # NOTE: stride info is in the 7th line after convDesc key.\n",
    "    strideInfo = stridePattern.match(lines[convDescLine + 7])\n",
    "    if strideInfo:\n",
    "        conv['stride_h'] = int(strideInfo.group('stride_h'))\n",
    "        conv['stride_w'] = int(strideInfo.group('stride_w'))\n",
    "    else:\n",
    "        print(\"ERROR: Padding dimensions not found at expected spot for block:{}.\".format(i+1))\n",
    "        conv['stride_h'] = conv['stride_w'] = -1\n",
    "    \n",
    "    # Dilation\n",
    "    dilPattern = re.compile(r'i!\\s+dilationA: type=int; val=\\[(?P<dil_h>\\d+),(?P<dil_w>\\d+)\\];')\n",
    "    # NOTE: dilation info is in the 8th line after convDesc key.\n",
    "    dilInfo = dilPattern.match(lines[convDescLine + 8])\n",
    "    if dilInfo:\n",
    "        conv['dil_h'] = int(dilInfo.group('dil_h'))\n",
    "        conv['dil_w'] = int(dilInfo.group('dil_w'))\n",
    "    else:\n",
    "        print(\"ERROR: Dilation dimensions not found at expected spot for block:{}.\".format(i+1))\n",
    "        conv['dil_h'] = conv['dil_w'] = -1\n",
    "    \n",
    "    # Group Count\n",
    "    groupPattern = re.compile(r'i!\\s+groupCount: type=int; val=(?P<groupCount>\\d+);')\n",
    "    # NOTE: groupcount info is in the 9th line after convDesc key.\n",
    "    groupInfo = groupPattern.match(lines[convDescLine + 9])\n",
    "    if groupInfo:\n",
    "        conv['groupCount'] = int(groupInfo.group('groupCount'))\n",
    "    else:\n",
    "        print(\"ERROR: Dilation dimensions not found at expected spot for block:{}.\".format(i+1))\n",
    "        conv['groupCount'] = -1\n",
    "    \n",
    "    ## ALGORITHM INFO\n",
    "    algoLine = [x for x in range(len(lines)) if 'algo' in lines[x]]\n",
    "    assert len(algoLine) == 1, \"multiple 'algo' in block:{}\".format(i+1)\n",
    "    algoLine = algoLine[0]\n",
    "    algoPattern = re.compile(r'i!\\s+algo: type=cudnnConvolutionFwdAlgo_t; val=(?P<algo>[a-zA-Z0-9_ ()]+);')\n",
    "    algoInfo = algoPattern.match(lines[algoLine])\n",
    "    if algoInfo:\n",
    "        conv['fwd_algo'] = algoInfo.group('algo')\n",
    "    else:\n",
    "        print(\"ERROR: Algorithm information not found at expected spot for block:{}.\".format(i+1))\n",
    "        conv['fwd_algo'] = \"UNKNOWN\"\n",
    "    # Add to list\n",
    "    convsInfo.append(conv)\n",
    "\n",
    "# for item in convsInfo:\n",
    "#     print(item)\n",
    "print(len(convsInfo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(convsInfo)\n",
    "print(len(df))\n",
    "df = df.drop_duplicates()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# print(df)\n",
    "distinctConv = df.apply(lambda item: \"std::make_tuple({},{},{},{},{},{},{},{},{},{},{},{})\".format(item['w'], item['h'], item['c'], item['n'],\n",
    "                                                                           item['k'], item['f_w'], item['f_h'], item['pad_w'],\n",
    "                                                                           item['pad_h'], item['stride_w'], item['stride_h'],\n",
    "                                                                           item['groupCount']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std::make_tuple(320,320,3,4,64,7,7,3,3,2,2,1),\n",
      "std::make_tuple(80,80,64,4,1536,1,1,0,0,1,1,1),\n",
      "std::make_tuple(80,80,1536,4,1536,3,3,1,1,1,1,32),\n",
      "std::make_tuple(80,80,1536,4,256,1,1,0,0,1,1,1),\n",
      "std::make_tuple(80,80,64,4,256,1,1,0,0,1,1,1),\n",
      "std::make_tuple(80,80,256,4,1536,1,1,0,0,1,1,1),\n",
      "std::make_tuple(80,80,256,4,3072,1,1,0,0,1,1,1),\n",
      "std::make_tuple(80,80,3072,4,3072,3,3,1,1,2,2,32),\n",
      "std::make_tuple(40,40,3072,4,512,1,1,0,0,1,1,1),\n",
      "std::make_tuple(80,80,256,4,512,1,1,0,0,2,2,1),\n",
      "std::make_tuple(40,40,512,4,3072,1,1,0,0,1,1,1),\n",
      "std::make_tuple(40,40,3072,4,3072,3,3,1,1,1,1,32),\n",
      "std::make_tuple(40,40,512,4,6144,1,1,0,0,1,1,1),\n",
      "std::make_tuple(40,40,6144,4,6144,3,3,1,1,2,2,32),\n",
      "std::make_tuple(20,20,6144,4,1024,1,1,0,0,1,1,1),\n",
      "std::make_tuple(40,40,512,4,1024,1,1,0,0,2,2,1),\n",
      "std::make_tuple(20,20,1024,4,6144,1,1,0,0,1,1,1),\n",
      "std::make_tuple(20,20,6144,4,6144,3,3,1,1,1,1,32),\n",
      "std::make_tuple(20,20,1024,4,12288,1,1,0,0,1,1,1),\n",
      "std::make_tuple(20,20,12288,4,12288,3,3,1,1,2,2,32),\n",
      "std::make_tuple(10,10,12288,4,2048,1,1,0,0,1,1,1),\n",
      "std::make_tuple(20,20,1024,4,2048,1,1,0,0,2,2,1),\n",
      "std::make_tuple(10,10,2048,4,12288,1,1,0,0,1,1,1),\n",
      "std::make_tuple(10,10,12288,4,12288,3,3,1,1,1,1,32)\n"
     ]
    }
   ],
   "source": [
    "# arglist = []\n",
    "# for item in df:\n",
    "#     string = \"std::make_tuple({},{},{},{},{},{},{},{},{},{},{},{})\".format(item['w'], item['h'], item['c'], item['n'],\n",
    "#                                                                            item['k'], item['f_w'], item['f_h'], item['pad_w'],\n",
    "#                                                                            item['pad_h'], item['stride_w'], item['stride_h'],\n",
    "#                                                                            item['groupCount'])\n",
    "#     arglist.append(string)\n",
    " \n",
    "allconvstr = \",\\n\".join(distinctConv)\n",
    "print(allconvstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(arglist))\n",
    "print(len(set(arglist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 320, 320])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 160, 160]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 160, 160]             128\n",
      "              ReLU-3         [-1, 64, 160, 160]               0\n",
      "         MaxPool2d-4           [-1, 64, 80, 80]               0\n",
      "            Conv2d-5         [-1, 1536, 80, 80]          98,304\n",
      "       BatchNorm2d-6         [-1, 1536, 80, 80]           3,072\n",
      "              ReLU-7         [-1, 1536, 80, 80]               0\n",
      "            Conv2d-8         [-1, 1536, 80, 80]         663,552\n",
      "       BatchNorm2d-9         [-1, 1536, 80, 80]           3,072\n",
      "             ReLU-10         [-1, 1536, 80, 80]               0\n",
      "           Conv2d-11          [-1, 256, 80, 80]         393,216\n",
      "      BatchNorm2d-12          [-1, 256, 80, 80]             512\n",
      "           Conv2d-13          [-1, 256, 80, 80]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 80, 80]             512\n",
      "             ReLU-15          [-1, 256, 80, 80]               0\n",
      "       Bottleneck-16          [-1, 256, 80, 80]               0\n",
      "           Conv2d-17         [-1, 1536, 80, 80]         393,216\n",
      "      BatchNorm2d-18         [-1, 1536, 80, 80]           3,072\n",
      "             ReLU-19         [-1, 1536, 80, 80]               0\n",
      "           Conv2d-20         [-1, 1536, 80, 80]         663,552\n",
      "      BatchNorm2d-21         [-1, 1536, 80, 80]           3,072\n",
      "             ReLU-22         [-1, 1536, 80, 80]               0\n",
      "           Conv2d-23          [-1, 256, 80, 80]         393,216\n",
      "      BatchNorm2d-24          [-1, 256, 80, 80]             512\n",
      "             ReLU-25          [-1, 256, 80, 80]               0\n",
      "       Bottleneck-26          [-1, 256, 80, 80]               0\n",
      "           Conv2d-27         [-1, 1536, 80, 80]         393,216\n",
      "      BatchNorm2d-28         [-1, 1536, 80, 80]           3,072\n",
      "             ReLU-29         [-1, 1536, 80, 80]               0\n",
      "           Conv2d-30         [-1, 1536, 80, 80]         663,552\n",
      "      BatchNorm2d-31         [-1, 1536, 80, 80]           3,072\n",
      "             ReLU-32         [-1, 1536, 80, 80]               0\n",
      "           Conv2d-33          [-1, 256, 80, 80]         393,216\n",
      "      BatchNorm2d-34          [-1, 256, 80, 80]             512\n",
      "             ReLU-35          [-1, 256, 80, 80]               0\n",
      "       Bottleneck-36          [-1, 256, 80, 80]               0\n",
      "           Conv2d-37         [-1, 3072, 80, 80]         786,432\n",
      "      BatchNorm2d-38         [-1, 3072, 80, 80]           6,144\n",
      "             ReLU-39         [-1, 3072, 80, 80]               0\n",
      "           Conv2d-40         [-1, 3072, 40, 40]       2,654,208\n",
      "      BatchNorm2d-41         [-1, 3072, 40, 40]           6,144\n",
      "             ReLU-42         [-1, 3072, 40, 40]               0\n",
      "           Conv2d-43          [-1, 512, 40, 40]       1,572,864\n",
      "      BatchNorm2d-44          [-1, 512, 40, 40]           1,024\n",
      "           Conv2d-45          [-1, 512, 40, 40]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 40, 40]           1,024\n",
      "             ReLU-47          [-1, 512, 40, 40]               0\n",
      "       Bottleneck-48          [-1, 512, 40, 40]               0\n",
      "           Conv2d-49         [-1, 3072, 40, 40]       1,572,864\n",
      "      BatchNorm2d-50         [-1, 3072, 40, 40]           6,144\n",
      "             ReLU-51         [-1, 3072, 40, 40]               0\n",
      "           Conv2d-52         [-1, 3072, 40, 40]       2,654,208\n",
      "      BatchNorm2d-53         [-1, 3072, 40, 40]           6,144\n",
      "             ReLU-54         [-1, 3072, 40, 40]               0\n",
      "           Conv2d-55          [-1, 512, 40, 40]       1,572,864\n",
      "      BatchNorm2d-56          [-1, 512, 40, 40]           1,024\n",
      "             ReLU-57          [-1, 512, 40, 40]               0\n",
      "       Bottleneck-58          [-1, 512, 40, 40]               0\n",
      "           Conv2d-59         [-1, 3072, 40, 40]       1,572,864\n",
      "      BatchNorm2d-60         [-1, 3072, 40, 40]           6,144\n",
      "             ReLU-61         [-1, 3072, 40, 40]               0\n",
      "           Conv2d-62         [-1, 3072, 40, 40]       2,654,208\n",
      "      BatchNorm2d-63         [-1, 3072, 40, 40]           6,144\n",
      "             ReLU-64         [-1, 3072, 40, 40]               0\n",
      "           Conv2d-65          [-1, 512, 40, 40]       1,572,864\n",
      "      BatchNorm2d-66          [-1, 512, 40, 40]           1,024\n",
      "             ReLU-67          [-1, 512, 40, 40]               0\n",
      "       Bottleneck-68          [-1, 512, 40, 40]               0\n",
      "           Conv2d-69         [-1, 3072, 40, 40]       1,572,864\n",
      "      BatchNorm2d-70         [-1, 3072, 40, 40]           6,144\n",
      "             ReLU-71         [-1, 3072, 40, 40]               0\n",
      "           Conv2d-72         [-1, 3072, 40, 40]       2,654,208\n",
      "      BatchNorm2d-73         [-1, 3072, 40, 40]           6,144\n",
      "             ReLU-74         [-1, 3072, 40, 40]               0\n",
      "           Conv2d-75          [-1, 512, 40, 40]       1,572,864\n",
      "      BatchNorm2d-76          [-1, 512, 40, 40]           1,024\n",
      "             ReLU-77          [-1, 512, 40, 40]               0\n",
      "       Bottleneck-78          [-1, 512, 40, 40]               0\n",
      "           Conv2d-79         [-1, 6144, 40, 40]       3,145,728\n",
      "      BatchNorm2d-80         [-1, 6144, 40, 40]          12,288\n",
      "             ReLU-81         [-1, 6144, 40, 40]               0\n",
      "           Conv2d-82         [-1, 6144, 20, 20]      10,616,832\n",
      "      BatchNorm2d-83         [-1, 6144, 20, 20]          12,288\n",
      "             ReLU-84         [-1, 6144, 20, 20]               0\n",
      "           Conv2d-85         [-1, 1024, 20, 20]       6,291,456\n",
      "      BatchNorm2d-86         [-1, 1024, 20, 20]           2,048\n",
      "           Conv2d-87         [-1, 1024, 20, 20]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 20, 20]           2,048\n",
      "             ReLU-89         [-1, 1024, 20, 20]               0\n",
      "       Bottleneck-90         [-1, 1024, 20, 20]               0\n",
      "           Conv2d-91         [-1, 6144, 20, 20]       6,291,456\n",
      "      BatchNorm2d-92         [-1, 6144, 20, 20]          12,288\n",
      "             ReLU-93         [-1, 6144, 20, 20]               0\n",
      "           Conv2d-94         [-1, 6144, 20, 20]      10,616,832\n",
      "      BatchNorm2d-95         [-1, 6144, 20, 20]          12,288\n",
      "             ReLU-96         [-1, 6144, 20, 20]               0\n",
      "           Conv2d-97         [-1, 1024, 20, 20]       6,291,456\n",
      "      BatchNorm2d-98         [-1, 1024, 20, 20]           2,048\n",
      "             ReLU-99         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-100         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-101         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-102         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-103         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-104         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-105         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-106         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-107         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-108         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-109         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-110         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-111         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-112         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-113         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-114         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-115         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-116         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-117         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-118         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-119         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-120         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-121         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-122         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-123         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-124         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-125         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-126         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-127         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-128         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-129         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-130         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-131         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-132         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-133         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-134         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-135         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-136         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-137         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-138         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-139         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-140         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-141         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-142         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-143         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-144         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-145         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-146         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-147         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-148         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-149         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-150         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-151         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-152         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-153         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-154         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-155         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-156         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-157         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-158         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-159         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-160         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-161         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-162         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-163         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-164         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-165         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-166         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-167         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-168         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-169         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-170         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-171         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-172         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-173         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-174         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-175         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-176         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-177         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-178         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-179         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-180         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-181         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-182         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-183         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-184         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-185         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-186         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-187         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-188         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-189         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-190         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-191         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-192         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-193         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-194         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-195         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-196         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-197         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-198         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-199         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-200         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-201         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-202         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-203         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-204         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-205         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-206         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-207         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-208         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-209         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-210         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-211         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-212         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-213         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-214         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-215         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-216         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-217         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-218         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-219         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-220         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-221         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-222         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-223         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-224         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-225         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-226         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-227         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-228         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-229         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-230         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-231         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-232         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-233         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-234         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-235         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-236         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-237         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-238         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-239         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-240         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-241         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-242         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-243         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-244         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-245         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-246         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-247         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-248         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-249         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-250         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-251         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-252         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-253         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-254         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-255         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-256         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-257         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-258         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-259         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-260         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-261         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-262         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-263         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-264         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-265         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-266         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-267         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-268         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-269         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-270         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-271         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-272         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-273         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-274         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-275         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-276         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-277         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-278         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-279         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-280         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-281         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-282         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-283         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-284         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-285         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-286         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-287         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-288         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-289         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-290         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-291         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-292         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-293         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-294         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-295         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-296         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-297         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-298         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-299         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-300         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-301         [-1, 6144, 20, 20]       6,291,456\n",
      "     BatchNorm2d-302         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-303         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-304         [-1, 6144, 20, 20]      10,616,832\n",
      "     BatchNorm2d-305         [-1, 6144, 20, 20]          12,288\n",
      "            ReLU-306         [-1, 6144, 20, 20]               0\n",
      "          Conv2d-307         [-1, 1024, 20, 20]       6,291,456\n",
      "     BatchNorm2d-308         [-1, 1024, 20, 20]           2,048\n",
      "            ReLU-309         [-1, 1024, 20, 20]               0\n",
      "      Bottleneck-310         [-1, 1024, 20, 20]               0\n",
      "          Conv2d-311        [-1, 12288, 20, 20]      12,582,912\n",
      "     BatchNorm2d-312        [-1, 12288, 20, 20]          24,576\n",
      "            ReLU-313        [-1, 12288, 20, 20]               0\n",
      "          Conv2d-314        [-1, 12288, 10, 10]      42,467,328\n",
      "     BatchNorm2d-315        [-1, 12288, 10, 10]          24,576\n",
      "            ReLU-316        [-1, 12288, 10, 10]               0\n",
      "          Conv2d-317         [-1, 2048, 10, 10]      25,165,824\n",
      "     BatchNorm2d-318         [-1, 2048, 10, 10]           4,096\n",
      "          Conv2d-319         [-1, 2048, 10, 10]       2,097,152\n",
      "     BatchNorm2d-320         [-1, 2048, 10, 10]           4,096\n",
      "            ReLU-321         [-1, 2048, 10, 10]               0\n",
      "      Bottleneck-322         [-1, 2048, 10, 10]               0\n",
      "          Conv2d-323        [-1, 12288, 10, 10]      25,165,824\n",
      "     BatchNorm2d-324        [-1, 12288, 10, 10]          24,576\n",
      "            ReLU-325        [-1, 12288, 10, 10]               0\n",
      "          Conv2d-326        [-1, 12288, 10, 10]      42,467,328\n",
      "     BatchNorm2d-327        [-1, 12288, 10, 10]          24,576\n",
      "            ReLU-328        [-1, 12288, 10, 10]               0\n",
      "          Conv2d-329         [-1, 2048, 10, 10]      25,165,824\n",
      "     BatchNorm2d-330         [-1, 2048, 10, 10]           4,096\n",
      "            ReLU-331         [-1, 2048, 10, 10]               0\n",
      "      Bottleneck-332         [-1, 2048, 10, 10]               0\n",
      "          Conv2d-333        [-1, 12288, 10, 10]      25,165,824\n",
      "     BatchNorm2d-334        [-1, 12288, 10, 10]          24,576\n",
      "            ReLU-335        [-1, 12288, 10, 10]               0\n",
      "          Conv2d-336        [-1, 12288, 10, 10]      42,467,328\n",
      "     BatchNorm2d-337        [-1, 12288, 10, 10]          24,576\n",
      "            ReLU-338        [-1, 12288, 10, 10]               0\n",
      "          Conv2d-339         [-1, 2048, 10, 10]      25,165,824\n",
      "     BatchNorm2d-340         [-1, 2048, 10, 10]           4,096\n",
      "            ReLU-341         [-1, 2048, 10, 10]               0\n",
      "      Bottleneck-342         [-1, 2048, 10, 10]               0\n",
      "AdaptiveAvgPool2d-343           [-1, 2048, 1, 1]               0\n",
      "          Linear-344                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 828,411,176\n",
      "Trainable params: 828,411,176\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.17\n",
      "Forward/backward pass size (MB): 6240.65\n",
      "Params size (MB): 3160.14\n",
      "Estimated Total Size (MB): 9401.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(3, 320, 320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
